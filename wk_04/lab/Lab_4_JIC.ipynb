{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#d35400'> Lab 4 | Simulation Experiments </font>\n",
    "Welcome to Lab 4! In this lab, we work with the implications of sampling from a probability distribution. We start off this lab by plotting a single normal distribution. We then sample the distribution and look for any relationships. After writing code, we look at the effects of sampling which relate to the potential for Type I and Type II errors. Lastly, we use bootstrapping to estimate a confidence interval for the median of a dataset.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"dog_donut_painting.jpg\" alt=\"Alt Text\", width=\"300\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#FF8C00'> About the Dataset </font>\n",
    "We start off this Jupyter Notebook by first examining the dataset from `dataset.csv`. We read this `.csv` file into a pandas data frame, upon which we then preview the dataset as a data frame by examining the first 5 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the numpy library\n",
    "import numpy as np\n",
    "\n",
    "# random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# importing the pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# importing the ipython library\n",
    "from IPython.display import display\n",
    "\n",
    "# reading the .csv file as a data frame\n",
    "dataset_df = pd.read_csv(\"dataset.csv\")\n",
    "\n",
    "# checking for any null values\n",
    "null_values = dataset_df.isnull().sum()\n",
    "display(\"Number of Null Values: \", null_values)\n",
    "\n",
    "# viewing the dataset\n",
    "display(dataset_df.head(5))\n",
    "\n",
    "# obtaining summary statistics\n",
    "display(dataset_df.info())\n",
    "\n",
    "# obtaining the description \n",
    "display(dataset_df.describe())\n",
    "\n",
    "# displaying the median values\n",
    "display(dataset_df.median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in this dataset, the three features collectively `variable_1`, `variable_2` and `variable_3`, are all of the data type `float64`.  There are 500 observations collectively. When we looked for any missing values, we happened to find none. Here are the summary statistics presented as follows:\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "\n",
    "| x                   | variable_1    | variable_2    | variable_3    |  \n",
    "|:-------------------:|:-------------:|:-------------:|:-------------:|  \n",
    "| mean                | -0.002162     | 0.822616      | 0.942165      |  \n",
    "| median              | 0.032894      | 0.797405      | 0.938862      |  \n",
    "| standard deviation  | 0.996963      | 1.006288      | 1.004571      |  \n",
    "| maximum             | 3.057066      | 4.150901      | 3.623870      |  \n",
    "| minimum             | -2.989982     | -2.428388     | -1.731287     |  \n",
    "| lower quartile      | -0.666835     | 0.176737      | 0.303725      | \n",
    "| upper quartile      | 0.649753      | 1.498369      | 1.551920      |  \n",
    "\n",
    "</div>\n",
    "\n",
    "When we look at the summary statistics above, we noticed that the mean and median for each of the features are very similar. As a result, it would be difficult to determine whether there is positive skew or negative skew. We can use the `scipy` library to confirm this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the skewness of each feature\n",
    "skew_values = dataset_df.skew()\n",
    "display(skew_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the skew values, we can confirm that it is unlikely that each of the variables have any skewness in them. For the `variable_1` feature, since the skewness is less than 0, then the tail is on the left side, which means there is left-skew, which is a negative skew. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = '#FF8C00'> Section 1 </font> | Plotting a Normal Distribution\n",
    "In this section, we are dealing with creating and plotting a normal distribution. Here are the following steps we take to achieve this:\n",
    "- [x] Import scipy and instantiate a normal distribution with µ = 0 and σ = 1\n",
    "- [x] Plot the pdf of this distribution (you may find .arange or .linspace useful).\n",
    "- [x] Plot the cdf of this distribution (you may ﬁnd .arange or .linspace useful).\n",
    "- [x] Using the inverse cdf and a unifrom distribution in the range of [0, 1), sample the distribution 1000 times. Plot the histogram of these 1000 observations (hint: lookup he probability point function)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = '#FF8C00'> Creating & Plotting a Probability Density Function </font>\n",
    "We start off by creating the probability density function. We initialize a normal distribution object using `norm()` and passing in the mean and standard deviation values. Next, we use `.linspace()` to create a array of equally spaced numbers from the `mean - 5*standard_deviation` to `mean + 5*standard_deviation`, which are plotted using 2000 generated values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the stats library\n",
    "from scipy.stats import norm\n",
    "\n",
    "# initializing the variables\n",
    "mean = 0\n",
    "standard_deviation = 1\n",
    "\n",
    "# instantiating a normal distribution with Mean 1 and SD 1 \n",
    "normal_distribution = norm(loc=mean, scale=standard_deviation)\n",
    "\n",
    "# generating the values required for a distribution (2000 is number of points)\n",
    "x_values = np.linspace(mean - 5*standard_deviation, mean + 5*standard_deviation, 2000);\n",
    "\n",
    "# calulcating the probability density function\n",
    "pdf_values = normal_distribution.pdf(x_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then move on to plotting the probability density function. Using the `matplotlib` library, we then plot the X values that we have generated against the density values that were generated after applying the `.pdf` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plotting the normal distribution\n",
    "plt.plot(x_values, pdf_values, color=\"red\")\n",
    "\n",
    "# adding the title and labels\n",
    "plt.title(\"PDF of a Normal Distrubtion Curve\")\n",
    "plt.xlabel(\"Values of X\")\n",
    "plt.ylabel(\"Density\")\n",
    "\n",
    "# saving the figure\n",
    "plt.savefig(r\"C:\\GitHub\\DataScience\\wk_04\\plots\\PDFNormalDistribution.png\")\n",
    "\n",
    "# displaying the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = '#FF8C00'> Creating & Plotting  Cumulative Distribution Function </font>\n",
    "Next, we move on to creating the cumulative distribution function. Using the normalized distribution object, which has a passed in mean and standard deviation parameters, and the array of equally spaced numbers using `.linspace()`, we then use `.cdf()` to calculate the cumulative distribution function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the cumulative distirbution function\n",
    "cdf_values = normal_distribution.cdf(x_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then move to plotting the cumulative distribution function. Using the `matplotlib` library, we then plot the X values that we have generated against the cumulative probabilities that were generated after applying the `.cdf` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plotting the cumulative distribution\n",
    "plt.plot(x_values, cdf_values, color=\"red\")\n",
    "\n",
    "# adding the title and labels\n",
    "plt.title(\"CDF of a Normal Distrubtion Curve\")\n",
    "plt.xlabel(\"Values of X\")\n",
    "plt.ylabel(\"Cumulative Probability\")\n",
    "\n",
    "# saving the figure\n",
    "plt.savefig(r\"C:\\GitHub\\DataScience\\wk_04\\plots\\CDFNormalDistribution.png\")\n",
    "\n",
    "# displaying the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = '#FF8C00'> The Inverse Cumulative Distribution Function </font>\n",
    "Next, we use the inverse cumulative distirbution function in the range of [0,1) and a sample of 1000 times. We then move on to plot a histogram of 1000 of these observations.  In order to find the inverse cumulative distribution, we use the `probability point function`, which returns the exact point where the probability of everything to the left is equal to the cumulative probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the uniform library from scipy\n",
    "from scipy.stats import uniform\n",
    "\n",
    "# generating random unform values between 0 and 1\n",
    "# np.random.rand() is the uniform distirbution\n",
    "random_uniform_values = np.random.rand(1000)\n",
    "\n",
    "\n",
    "# calculating the probability point function values using mean 1 and SD 1\n",
    "ppf_values = normal_distribution.ppf(random_uniform_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we then move on to plotting the probability point function as a histogram. Using the `matplotlib` library, we then plot the probability value on the x-axis and the cumulative probability on the y-axis after applying the `.ppf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting a histogram\n",
    "plt.hist(ppf_values)\n",
    "\n",
    "# setting up the titles and x and y headings\n",
    "plt.title(\"Histogram of a Probability Point Function\")\n",
    "plt.xlabel(\"Probability\")\n",
    "plt.ylabel(\"Freqeuncy\")\n",
    "\n",
    "# saving the figure\n",
    "plt.savefig(r\"C:\\GitHub\\DataScience\\wk_04\\plots\\PPFHistogram.png\")\n",
    "\n",
    "# displaying the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#FF8C00'> Sources Used For Section One </font>\n",
    "- https://discovery.cs.illinois.edu/learn/Polling-Confidence-Intervals-and-Hypothesis-Testing/Python-Functions-for-Random-Distributions/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = '#FF8C00'> Section 2 </font> | Method for Sampling\n",
    "In this section, we create a function that takes in 2 parameters which in turn returns a list of length n elements. We use this function generate data and plot histograms. For each of these histograms, we plot a normal probability density function over the histogram. Here are the following steps to take:\n",
    "- [x] Using the results from problem 1, create a function that accepts 2 parameters - a scipy class distribution object and an integer n. Here, n is the number of samples to take from the distribution. This method should return a list of length n elements, where each element is the value of a single sample \n",
    "- [x] Use your function to generate the following data and make histograms for 10, 20, 40, 80 and 160 samples\n",
    "- [x] For each one of these histograms, directly plot the normal pdf over the histogram (Hint: seaborns .histplot with stat=”probability” may be useful).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = '#FF8C00'> Creating the Function </font>\n",
    "Here, we create a function that accepts 2 parameters, a `scipy` class distribution object and a integer `n`. Here, `n` is the number of samples to take from the distribution. This method should return a list of length n elements, where each element is the value of a single sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that accepts two parameters\n",
    "def sample_values(distribution, n):\n",
    "\n",
    "    # generating random unform values between 0 and 1\n",
    "    # np.random.rand() is the uniform distirbution\n",
    "    random_uniform_values = np.random.rand(n)\n",
    "\n",
    "    # calculating the probability point function values using mean 1 and SD 1\n",
    "    ppf_values = distribution.ppf(random_uniform_values)\n",
    "\n",
    "    # returning the ppf values\n",
    "    return ppf_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = '#FF8C00'> Plotting the Histograms </font>\n",
    "We use the function we created in the cell earlier to generate data and make histograms for each. Here are the details as follows:\n",
    "- Normal distribution - 10 samples\n",
    "- Normal distribution - 20 samples\n",
    "- Normal distribution - 40 samples\n",
    "- Normal distribution - 80 samples\n",
    "- Normal distribution - 160 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the mean, standard deviation\n",
    "mean = 0\n",
    "standard_deviation = 1\n",
    "\n",
    "# instantiating the normal distribution\n",
    "normal_dist = norm(loc=mean, scale=standard_deviation)\n",
    "\n",
    "# list of sample sizes\n",
    "sample_sizes = [10, 20, 40, 80, 160]\n",
    "\n",
    "# for looping through the sample sizes\n",
    "for size in sample_sizes:\n",
    "\n",
    "    # a list of generated values\n",
    "    generated_values = sample_values(normal_dist, size)\n",
    "\n",
    "    # generating a histogram\n",
    "    plt.hist(generated_values)\n",
    "\n",
    "    # setting the title, x label and y label\n",
    "    plt.title(f'Normal Distribution Histogram of {size} Samples')\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    # defining the file name\n",
    "    file_name = f'NDH of {size} Samples'\n",
    "\n",
    "    # saving the figure\n",
    "    plt.savefig(rf\"C:\\GitHub\\DataScience\\wk_04\\plots\\{file_name}.png\")\n",
    "    \n",
    "    # displaying the figures\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = '#FF8C00'> Normal PDF over Histogram </font>\n",
    "Lastly, for each one of the histograms, we directly plot the normal probability density function over each of the generated histograms. We achieve this by using the `seaborn` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the seaborn library\n",
    "import seaborn as sns\n",
    "\n",
    "# importing the scipy libraru\n",
    "import scipy\n",
    "\n",
    "# for looping through the sample sizes\n",
    "for size in sample_sizes:\n",
    "    generated_values = sample_values(normal_dist, size)\n",
    "\n",
    "    # histogram plot using seaborn\n",
    "    ax =  sns.histplot(generated_values, kde=False, stat='density', label='samples')\n",
    "\n",
    "    # extracting end points for x-axis\n",
    "    x0, x1 = ax.get_xlim()\n",
    "    x_pdf = np.linspace(x0, x1, 100)\n",
    "    y_pdf = scipy.stats.norm.pdf(x_pdf)\n",
    "\n",
    "    # plotting the normal probability density function\n",
    "    ax.plot(x_pdf, y_pdf, 'r', lw=2, label='pdf')\n",
    "\n",
    "    # displaying the legend\n",
    "    ax.legend()\n",
    "\n",
    "    # displaying the title and x and y labels\n",
    "    plt.title(f'PDF over Histogram of {size} Samples')\n",
    "    plt.xlabel(\"Value\")\n",
    "\n",
    "    # defining the file name\n",
    "    file_name = f'PDFHistogram {size} Samples'\n",
    "\n",
    "    # saving the figure\n",
    "    plt.savefig(rf\"C:\\GitHub\\DataScience\\wk_04\\plots\\{file_name}.png\")\n",
    "\n",
    "    # displaying the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#FF8C00'> Sources Used For Section Two </font>\n",
    "- https://stackoverflow.com/questions/30453097/getting-the-parameter-names-of-scipy-stats-distributions\n",
    "- https://stackoverflow.com/questions/52908925/how-to-add-a-standard-normal-pdf-over-a-seaborn-histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = '#FF8C00'> Section 3 </font> | Type 1 Errors\n",
    "The section involves generating two sample groups from normal distributions, calculating the effect size as the absolute difference in sample means, and repeating this process for k iterations. After generating 1000 iterations, a histogram of the effect sizes is plotted, and the sample groups corresponding to the largest observed effect size are visualized. Here are the following steps to take:\n",
    "- [x] Using the function from the previous problem, draw two sample groups using the same distribution and the same n. E.g. the ﬁrst group should be 25 samples drawn from a normal distribution with µ = 0 and σ = 1, the second group should be 25 diﬀerent samples drawn from a normal distribution with µ = 0 and σ = 1.\n",
    "- [x] Calculate the estimated eﬀect size between these two groups. (the absolute value of the diﬀerence between group 1 sample mean and group 2 sample mean).\n",
    "- [x] Create a function that accepts two distribution objects, two values of n for number of samples to draw from the distributions, and an integer k which is the number of times to loop the sample generations. This method should draw from each distribution n times and repeat this process for k iterations. The method should return three lists\n",
    "that are k elements long. The elements in the ﬁrst list are the estimated eﬀect sizes at iteration k between the drawn samples of the two distributions. Each element in the second list is a list of the drawn samples from distribution 1 at iteration k. Each element in the third list is a list of the drawn samples from distribution 2 at iteration k.\n",
    "- [x] Use the deﬁned method to generate two sample groups, each with 25 observations, for 1000 iterations. Plot a histogram of the eﬀect sizes. \n",
    "- [x] Find the index of the largest observed eﬀect size. Use the index to plot the histograms of the two associated sample groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = '#FF8C00'> Drawing Two Sample Groups </font>\n",
    "We start off by using the previous function to draw two sample groups using the same distribution and the same n. For example:\n",
    "- the first group should be 25 samples drawn from a normal distribution with mean = 0 and SD = 1\n",
    "- the second group should be 25 different samples drawn from a normal distribution with mean = 0 and SD = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the random library\n",
    "import random\n",
    "\n",
    "# maintaining a constant n\n",
    "n_size = 25\n",
    "\n",
    "# instantiating the normal distribution\n",
    "normal_dist_one = norm(loc=0, scale=1)\n",
    "\n",
    "# generating random unform values between 0 and 1\n",
    "# np.random.rand() is the uniform distirbution\n",
    "random_uniform_values_one = np.random.rand(n_size)\n",
    "\n",
    "# calculating the probability point function values using mean 1 and SD 1\n",
    "ppf_values_one = normal_dist_one.ppf(random_uniform_values_one)\n",
    "\n",
    "# instantiating the normal distribution\n",
    "normal_dist_two = norm(loc=0, scale=1)\n",
    "\n",
    "# generating random unform values between 0 and 1\n",
    "# np.random.rand() is the uniform distirbution\n",
    "random_uniform_values_two = np.random.rand(n_size)\n",
    "\n",
    "# calculating the probability point function values using mean 1 and SD 1\n",
    "ppf_values_two = normal_dist_two.ppf(random_uniform_values_two)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = '#FF8C00'> Estimated Effect Size </font>\n",
    "Next, we calculate the estimated effect size between the two groups. We calculate this by finding the absolute value of the difference between group 1 sample mean and group 2 sample mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the mean of group one sample\n",
    "mean_one = sum(ppf_values_one) / len(ppf_values_one)\n",
    "print(\"Mean of Random Sample One: \", mean_one)\n",
    "\n",
    "# finding the mean grop two sample\n",
    "mean_two = sum(ppf_values_two) / len(ppf_values_two)\n",
    "print(\"Mean of Random Sample Two: \", mean_two)\n",
    "\n",
    "# finding the absolute value of the difference\n",
    "difference = mean_two - mean_one\n",
    "absolute_difference = abs(difference)\n",
    "print(\"Estimated Effect Size: \", absolute_difference)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = '#FF8C00'> Creating a Function </font>\n",
    "We now create a function that accepts two distribution objects, two values of n for number of samples to draw from the distributions, and a integer k which is the number of times to loop the sample generations.\n",
    "\n",
    "This method should draw from each distribution n times and repeat this process for k iterations. As a result, this method should return three lists that are k elements long. \n",
    "\n",
    "- The elements in the first list are the estimated effect sizes at iteration k between the drawn samples of the two distributions\n",
    "- Each element in the second list is a list of the drawn samples from distribution 1 at iteration k\n",
    "- Each element in the third list is a list of the drawn samples from distribtuion 2 at iteration k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function designed to take in 4 parameters\n",
    "def two_distribution(distribution_one, distribution_two, n_one, n_two, k):\n",
    "    \"\"\"\n",
    "    distribution one - a distribution object\n",
    "    distribution two - a distribution object\n",
    "    n_one - a number of samples\n",
    "    n_two - a number of samples\n",
    "    k - number of times to loop\n",
    "    \"\"\"\n",
    "\n",
    "    # intitializing all the required lists\n",
    "    effect_size = []\n",
    "    sample_one_array = []\n",
    "    sample_two_array = []\n",
    "\n",
    "    # for looping through the values of k\n",
    "    for _ in range(k):\n",
    "\n",
    "        # generating random unform values between 0 and 1\n",
    "        # np.random.rand() is the uniform distirbution\n",
    "        random_uniform_values_one = np.random.rand(n_one)\n",
    "\n",
    "        # calculating the probability point function values using mean 1 and SD 1\n",
    "        ppf_values_one = distribution_one.ppf(random_uniform_values_one)\n",
    "\n",
    "        # appending the ppf values one to the sample_one_array\n",
    "        sample_one_array.append(ppf_values_one)\n",
    "\n",
    "        # generating random unform values between 0 and 1\n",
    "        # np.random.rand() is the uniform distirbution\n",
    "        random_uniform_values_two = np.random.rand(n_two)\n",
    "\n",
    "        # calculating the probability point function values using mean 1 and SD 1\n",
    "        ppf_values_two = distribution_two.ppf(random_uniform_values_two)\n",
    "\n",
    "        # appending the ppf values one to the sample_one_array\n",
    "        sample_two_array.append(ppf_values_two)\n",
    "\n",
    "        # calculating the effect size\n",
    "        mean_one = np.mean(sample_one_array)\n",
    "        mean_two = np.mean(sample_two_array)\n",
    "        absolute_value = abs(mean_two - mean_one)\n",
    "        effect_size.append(absolute_value)\n",
    "\n",
    "    \n",
    "    # returning the three lists\n",
    "    return effect_size, sample_one_array, sample_two_array\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = '#FF8C00'> Generating Two Sample Groups </font>\n",
    "We now use our defined method to generate two sample groups, each with 25 observations for 1000 iterations. We then move to plot a histogram of the effect sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declaring the new mean and standard deviation variable\n",
    "mean_one = 0\n",
    "standard_deviation_one = 1\n",
    "\n",
    "mean_two = 0\n",
    "standard_deviation_two = 1\n",
    "\n",
    "# caling the normal distribution objects\n",
    "normal_dist_one = norm(loc=mean_one, scale=standard_deviation_one)\n",
    "normal_dist_two = norm(loc=mean_two, scale=standard_deviation_two)\n",
    "\n",
    "# storing the outputs of the functions\n",
    "effect_size, sample_one_array, sample_two_array = two_distribution(normal_dist_one, normal_dist_two, 25, 25, 1000)\n",
    "\n",
    "# plotting a histogram of the effect size\n",
    "plt.hist(effect_size)\n",
    "\n",
    "# adding the title, x and y axis label\n",
    "plt.xlabel(\"Effect Size\")\n",
    "plt.ylabel(\"Iterations\")\n",
    "plt.title(\"Effect Size vs Iterations\")\n",
    "\n",
    "# saving the figure\n",
    "plt.savefig(rf\"C:\\GitHub\\DataScience\\wk_04\\plots\\EffectSizeIterations.png\")\n",
    "\n",
    "\n",
    "# displaying the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = '#FF8C00'> Histogram of Sample Groups </font>\n",
    "Next, we find the index of the largest observed effect size. We use the index to plot the histograms of the of the two associated sample groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding largest observed effect size\n",
    "max_number = np.max(effect_size)\n",
    "\n",
    "# finding the index number\n",
    "index_number = effect_size.index(max_number)\n",
    "\n",
    "# printing out the maximum number\n",
    "print(\"Maximum Number is: \", max_number)\n",
    "\n",
    "# variable for storing the index number\n",
    "index = 0\n",
    "\n",
    "# finding the index of the effect size\n",
    "for i in range(0, len(effect_size)):\n",
    "    print(\"Effect Size is: \", effect_size[i],\"and index is \", i)\n",
    "    if effect_size[i] == max_number:\n",
    "        index += i\n",
    "\n",
    "# printing out the index number\n",
    "print(\"Index Number: \", index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the histogram of both samples using the index number\n",
    "plt.hist(sample_one_array[index_number])\n",
    "plt.hist(sample_two_array[index_number])\n",
    "\n",
    "# putting in the title, x label and y label\n",
    "plt.title(\"Histogram of Largest Effect Size of Two Samples\")\n",
    "plt.xlabel(\"Effect Size\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "# saving the figure\n",
    "plt.savefig(rf\"C:\\GitHub\\DataScience\\wk_04\\plots\\HistogramSample.png\")\n",
    "\n",
    "# displaying the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#FF8C00'> Sources Used For Section Three </font>\n",
    "- https://www.geeksforgeeks.org/randomly-select-n-elements-from-list-in-python/\n",
    "- https://www.geeksforgeeks.org/remove-item-from-list-in-python/\n",
    "- https://www.physiotutors.com/wiki/effect-size/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = '#FF8C00'> Section 4 </font> | Type 2 Errors\n",
    "In this section, we work with Type II errors. We generate two normal distributions, then run 1000 iterations of sampling 25 values from each. We compute the effect sizes, plot their histogram, and identify the minimum effect size index. Finally, visualize the corresponding sample groups using histograms. Here are the following steps to take:\n",
    "- [x] Instantiate another normal distribution object with parameters µ = 1 and σ = 1\n",
    "- [x] Using the function developed in problem 3.3., repeat the experiment in part 3.4. using two diﬀerent distributions. This time the ﬁrst sample group should be 1000 iterations of 25 samples drawn from a normal distribution with µ = 0 and σ = 1 and the second group should be 1000 iterations of 25 diﬀerent samples drawn from a normal distribution with µ = 1 and σ = 1.  \n",
    "- [ ] Plot the histogram of the estimated eﬀect sizes. Find the index of the minimum eﬀect size and plot the two sample groups associated with this index using a histogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = '#FF8C00'> Instantiating Normal Distributions </font>\n",
    "We start off by instantiating the normal distributions. The first one should have the mean set to 0 and standard deviation set to 1. The second one should have the mean set to 1 and standard deviation set to 1. Next, we run 1000 iterations of sampling 25 values each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the mean and standard deviation\n",
    "mean_one = 0\n",
    "standard_deviation_one = 1\n",
    "\n",
    "mean_two = 1\n",
    "standard_deviation_two = 1\n",
    "\n",
    "\n",
    "# caling the normal distribution objects\n",
    "normal_dist_one = norm(loc=mean_one, scale=standard_deviation_one)\n",
    "normal_dist_two = norm(loc=mean_two, scale=standard_deviation_two)\n",
    "\n",
    "# storing the outputs of the functions\n",
    "effect_size, sample_one_array_min, sample_two_array_min = two_distribution(normal_dist_one, normal_dist_two, 25, 25, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = '#FF8C00'> Plotting the Histogram </font>\n",
    "Next, we plot the histogram of the estimated effect sizes. We find the index of the minimum effect size and plot the two sample groups associated with this index using a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the estimated effect sizes as a histogram\n",
    "plt.hist(effect_size)\n",
    "\n",
    "# naming the title and x and y axes\n",
    "plt.title(\"Histogram of Effect Sizes\")\n",
    "plt.xlabel(\"Effect Size\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "# saving the figure\n",
    "plt.savefig(rf\"C:\\GitHub\\DataScience\\wk_04\\plots\\HistogramIterations.png\")\n",
    "\n",
    "# displaying the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding smallest observed effect size\n",
    "min_number = np.min(effect_size)\n",
    "\n",
    "# finding the index number\n",
    "index_number_small = effect_size.index(min_number)\n",
    "\n",
    "# printing out the maximum number\n",
    "print(\"Minimum Number is: \", min_number)\n",
    "\n",
    "# variable for storing the index number\n",
    "index = 0\n",
    "\n",
    "# finding the index of the effect size\n",
    "for i in range(0, len(effect_size)):\n",
    "    print(\"Effect Size is: \", effect_size[i],\"and index is \", i)\n",
    "    if effect_size[i] == min_number:\n",
    "        index += i\n",
    "\n",
    "# printing out the index number\n",
    "print(\"Index Number: \", index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the histogram of both samples using the index number\n",
    "plt.hist(sample_one_array_min[index_number_small])\n",
    "plt.hist(sample_two_array_min[index_number_small])\n",
    "\n",
    "# putting in the title, x label and y label\n",
    "plt.title(\"Histogram of Smallest Effect Size of Two Samples\")\n",
    "plt.xlabel(\"Effect Size\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "# saving the figure\n",
    "plt.savefig(rf\"C:\\GitHub\\DataScience\\wk_04\\plots\\HistogramSample.png\")\n",
    "\n",
    "# displaying the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#FF8C00'> Sources Used For Section Four </font>\n",
    " - Office Hours with Dr. Bukowy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = '#FF8C00'> Section 5 </font> | Bootstrapping\n",
    "In thi section, we implement a simple bootstrapping algorithm from scratch to estimate the confidence interval around the median of a data set. A confidence interval is a measure of how much the median is expected to vary. Here are the following deliverables:\n",
    "- [ ]  Load in the data set from the assignment on canvas called some data.csv. There should be three variables in this data set. We will produce bootstrapped estimates of the conﬁdence interval around the median for each of the three variables.\n",
    "- [ ]  For each of the three variables perform the following. For 1000 iterations: create a new bootstrap data set of 500 obsverations by sampling with replacement from the 500 observations generated in the data set; for each bootstrap data set compute the median and record it; sort the recorded values you have estimated for the median; take the 25th and 975th values as the bounds of your conﬁdence interval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = '#FF8C00'> Loading the Dataset </font>\n",
    "We begin by loading the dataset form `data.csv`. We display this dataset using `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# viewing the dataset of the first 5 rows\n",
    "dataset_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = '#FF8C00'> Bootstrapping </font>\n",
    "We have three variables that consists of 500 values. For each of these variables, we will use a for loop of 1,000 iterations to randomly sample the 500 values. We then compute the median of the resampled data and store the median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting variables to lists\n",
    "variable_one = dataset_df['variable_1'].values.tolist()\n",
    "variable_two = dataset_df['variable_2'].values.tolist()\n",
    "variable_three = dataset_df['variable_3'].values.tolist()\n",
    "\n",
    "# number of medians list\n",
    "median_one = []\n",
    "median_two = []\n",
    "median_three = []\n",
    "\n",
    "# for looping using iterations for variable_one\n",
    "iterations = 1000\n",
    "for i in range(iterations):\n",
    "    sample = np.random.choice(variable_one)\n",
    "    median_one.append(np.median(sample))\n",
    "\n",
    "# for looping using iterations for variable_two \n",
    "for i in range(iterations):\n",
    "    sample = np.random.choice(variable_two)\n",
    "    median_two.append(np.median(sample))\n",
    "        \n",
    "# for looping using iterations for variable_three\n",
    "for i in range(iterations):\n",
    "    sample = np.random.choice(variable_three)\n",
    "    median_three.append(np.median(sample))\n",
    "\n",
    "# printing out the median lists\n",
    "print(\"Median One: \", median_one, \"\\n\")\n",
    "print(\"Median Two: \", median_two, \"\\n\")\n",
    "print(\"Median Three: \", median_three, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = '#FF8C00'> Computing Confidence Interval </font>\n",
    "After 1,000 iterations, we will have 1,000 median values for each variable. We sort these median values in ascending order. Next, we find the 25th value and the 975th value. These values tell us the range within which the true median is likely to fall 95% of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting the median values in ascending order\n",
    "median_sort_one = np.sort(median_one)\n",
    "median_sort_two = np.sort(median_two)\n",
    "median_sort_three = np.sort(median_three)\n",
    "\n",
    "# finding the confidence interval of median_one\n",
    "lower_bound_one = median_sort_one[25]\n",
    "upper_bound_one = median_sort_one[975]\n",
    "\n",
    "# finding the confidence interval of median_two\n",
    "lower_bound_two = median_sort_two[25]\n",
    "upper_bound_two = median_sort_two[975]\n",
    "\n",
    "# finding the confidence interval of median_three\n",
    "lower_bound_three = median_sort_three[25]\n",
    "upper_bound_three = median_sort_three[975]\n",
    "\n",
    "# printing out the lower bounds for one, two, three\n",
    "print(\"Lower Bound One: \", lower_bound_one, \"\\n\")\n",
    "print(\"Lower Bound Two: \", lower_bound_two, \"\\n\")\n",
    "print(\"Lower Bound Three: \", lower_bound_three, \"\\n\")\n",
    "\n",
    "# printing out the upper bounds for one, two, three\n",
    "print(\"Upper Bound One: \", upper_bound_one, \"\\n\")\n",
    "print(\"Upper Bound Two: \", upper_bound_two, \"\\n\")\n",
    "print(\"Upper Bound Three: \", upper_bound_three, \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataScienceEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
