{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is Pandas?\n",
    "- Library for manupulating tabular data\n",
    "- Pandas is a package built on Numpy\n",
    "- Primarily used for cleaning and restructuring data in preperation for plotting and modeling\n",
    "- 2 primary data structures\n",
    "    - Series - 1D, columns of data\n",
    "    - DataFrames - 2D, tables of data\n",
    "- If you are familiar with SQL, this [page](https://pandas.pydata.org/pandas-docs/stable/getting_started/comparison/comparison_with_sql.html) shows how similar SQL operations can be performed in Pandas\n",
    "    - [Pandas Cheatsheets](https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf)\n",
    "    - [Pandas Documentation](http://pandas.pydata.org/pandas-docs/stable/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy vs Pandas\n",
    "\n",
    "|Numpy|Pandas|\n",
    "|:-----|:------|\n",
    "|- Any dimension|- Limted to 1 (Series) or 2 (DataFrame) dimensions|\n",
    "|- Indexing by position (e.g., row or column)|- Indexing primarily by position or by labels|\n",
    "|- Stuck with single datatype (e.g., int, float, etc...)|- Each column has its own type (heteregeneous)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benefits\n",
    "- Efficient storage and processing of data.\n",
    "- Includes many built in functions for data transformation, aggregations, and plotting.\n",
    "- Great for exploratory work.\n",
    "\n",
    "\n",
    "### Caveats\n",
    "- Does not scale terribly well to large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pandas Series\n",
    "- A Pandas Series is a one-dimensional array of indexed orlabeled data. \n",
    "- Pandas series can be seen as:\n",
    "    - a vector array (one dimensional NumPy array), but with an index or label for each value.\n",
    "    - a Python dictionary but with an order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_1 = pd.Series([5775,373,17,33], index=[\"first\",\"second\",\"third\",\"fourth\"])\n",
    "print(series_1)\n",
    "print(\"\\n\")\n",
    "print(list(series_1.index)) #get the list of indices or labels\n",
    "print(series_1.values)\n",
    "print(series_1[\"second\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "population_dict = {\"California\":38332521, \"Florida\":19552860, \n",
    "                   \"Illinois\":12882135, \"New York\":19651127, \"Texas\":26448193}\n",
    "series_2 = pd.Series(population_dict)\n",
    "print(series_2)\n",
    "print(\"\\n\")\n",
    "print(series_2.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_3 = pd.Series([4.1, 9, 3, 7])\n",
    "print(series_3)\n",
    "print(series_3.index) # by default, indices starting from 0 are giving to values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Series Methods**:\n",
    "- Same NumPy methods: .mean(), .sum(), .max(), .min()\n",
    "- We can convert a Pandas series to a Numpy array using: .values or to_numpy()\n",
    "- We can convert a Pandas series to a list using: .tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## additional functionalities\n",
    "series_3.values # trasnforms series into numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pandas DataFrames\n",
    "\n",
    "A Pandas DataFrame can be thought as:\n",
    "- mulitple Pandas series with matching index;\n",
    "- 2-dimentional array with labeled rows and columns.\n",
    "\n",
    "We can create our own data frame or read tabular data from a file and store it in a Pandas dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas IO tools\n",
    "\n",
    "Pandas provide a set of readers and writers for various file types. In this [link](https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html), you can check the available readers and writers. In this demo, we will focus on `pd.read_csv()` and `.to_csv()`.\n",
    "\n",
    "**Read from csv file**: `pd.read_csv()`:\n",
    "- It reads a comma-separated values (csv) file into DataFrame.\n",
    "- It takes as input a path to a local file or a valid URL.\n",
    "- The default delimiter is comma, if the values are separated by other characters we need to specify the delimiter using the parameter sep or delimiter.\n",
    "- The default behavior is to infer the column names from the first line, if the csv file does not contain the column names, we need to set header=None.\n",
    "\n",
    "Please check [https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html#pandas.read_csv](here) for more info about what the function expects as parameters: \n",
    "\n",
    "`pandas.read_csv(filepath_or_buffer, *,` **sep=_NoDefault.no_default**, `delimiter=None, `**header='infer'**, `names=_NoDefault.no_default, index_col=None, `**usecols=None**, `squeeze=None, prefix=_NoDefault.no_default, mangle_dupe_cols=True, dtype=None, engine=None, converters=None, true_values=None, false_values=None, skipinitialspace=False,` **skiprows=None**, `skipfooter=0, nrows=None, na_values=None, keep_default_na=True, na_filter=True, verbose=False, skip_blank_lines=True, parse_dates=None, infer_datetime_format=False, keep_date_col=False, date_parser=None, dayfirst=False, cache_dates=True, iterator=False, chunksize=None, compression='infer', thousands=None, decimal='.', lineterminator=None, quotechar='\"', quoting=0, doublequote=True, escapechar=None, comment=None, encoding=None, encoding_errors='strict', dialect=None, error_bad_lines=None, warn_bad_lines=None, on_bad_lines=None, delim_whitespace=False, low_memory=True, memory_map=False, float_precision=None, storage_options=None)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"sales_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)\n",
    "print(data.columns) # get the labels of the columns\n",
    "print(data.index) # get the labels of the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.head(10) # read the first 10 rows\n",
    "#data.tail(10) # read the last 10 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quick Data Exploration**\n",
    ".info(), .describe(), .value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe() # summary statistics of numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How can we summarize a categorical column?\n",
    "\n",
    "#data['STATUS'].unique() # returns the different categories\n",
    "#data['STATUS'].value_counts() # returns the counts for each category\n",
    "#data['STATUS'].value_counts(normalize=True) # returns the percentage for each category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection Methods\n",
    "- How can we select a column or multiple columns?\n",
    "- How can we select a row or multiple rows?\n",
    "- How can we select one value?\n",
    "- How can we select a portion of the DataFrame? (i.e., subsetting)\n",
    "\n",
    "\n",
    "We will discuss: access by label, access by position and boolean masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column Selection - by Label\n",
    "\n",
    "data[\"STATUS\"] # returns a Pandas series that has the same index of the original DataFrame\n",
    "#data[[\"STATUS\",\"SALES\"]] # returns a Pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas provide some special indexer attributes:\n",
    "- df.loc is label based. This indexer works with row and column indices / labels.\n",
    "- df.iloc is positionally based. This indexer accepts integers and integer slices, and essentially treats the data frame as if it were a simple matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Row & column selection - by Label using `.loc[]`**\n",
    "\n",
    "Possible Syntaxes: \n",
    "- `data.loc[row_label, column_label]` \n",
    "- `data.loc[row_label]`\n",
    "- `data.loc[multiple row labels, multiple column labels]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[2, \"SALES\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[[3, 8, 1], [\"SALES\", \"STATUS\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#slicing by labels\n",
    "data.loc[3:6, \"QUANTITYORDERED\":\"STATUS\"] # both limits are inclusive\n",
    "#data.loc[:,[\"SALES\", \"STATUS\"]] # select all rows\n",
    "#data.loc[1,:] # select all columns, return the row that has label 1\n",
    "#data.loc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Row & Column Selection - by position using `.iloc[]`**\n",
    "\n",
    "Syntax: `.iloc[row number, column number]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[0, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[0:3, 1:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify a range of position\n",
    "#print(data.iloc[1:3, 0:2]) # rows by positions: 1,2 and columns: 0, 1 \n",
    "#print(data.iloc[:, 0:2]) # all rows, columns: 0, 1\n",
    "#print(data.iloc[1:3, :]) # rows: 1, 2 and all columns\n",
    "#print(data.iloc[:3, 1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Row & Column Selection - Filtering or boolean masking**\n",
    "\n",
    "Let's say we only want to select orders for which the status is shipped.\n",
    "\n",
    "`data.loc[]` expects row labels or list of booleans that indicate which rows to select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data[\"STATUS\"]==\"Shipped\"] #or data[data[\"STATUS\"]==\"Shipped\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multiple conditions\n",
    "#data.loc[(data[\"SALES\"]>=2000) & (data[\"STATUS\"]==\"Shipped\")]\n",
    "\n",
    "#another way of writing boolean .isin()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a New Column\n",
    "\n",
    "Let's create a column that contains boolean values, such that if the order is shipped the column contains True for that particular order and False otherwise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"STATUS_BINARY\"] = data[\"STATUS\"]==\"Shipped\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying a column\n",
    "- Convert data types - may need to specify function for parsing /conversion\n",
    "- Cleaning data\n",
    "- Extracting fields from complex types\n",
    "    - e.g., hour, month, etc... from date times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: \n",
    "\n",
    "Let's say we want to apply a process a column. We can use the function `.apply()`. It takes in a function that specifies how we want to process each entry in the specified colum. \n",
    "\n",
    "In the column ORDERDATE, we see we have the date and time separated by space. For each entry, we just want to extract the data and update the column ORDERDATE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example\n",
    "\n",
    "#define the function that will be apply to each entry in the column\n",
    "def extract_date(date):\n",
    "    return date.split(\" \")[0]\n",
    "\n",
    "data[\"ORDERDATE\"] = data['ORDERDATE'].apply(extract_date)\n",
    "\n",
    "#data[\"ORDERDATE\"] = data['ORDERDATE'].apply(lambda date: date.split(\" \")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"ORDERDATE\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping a Column\n",
    "- I prefer to use the drop() method becuase it returns a DataFrame object, so it work with chaining:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = data.drop(columns=[\"STATUS_BINARY\"])\n",
    "\n",
    "#data.drop(columns=[\"STATUS_BINARY\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Operations on the DataFrame\n",
    "\n",
    "- Use same NumPy methods: .mean(), .sum(), .max(), .min()\n",
    "- By default, these functions return the result from each column; for instance .sum() would sum the values from all rows for each column.\n",
    "- To make them return the values from each row, we set the parameter axis=1 \n",
    "- The default value for axis is: axis=0 or axis='rows'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = [ 'QUANTITYORDERED', 'PRICEEACH', 'SALES']\n",
    "\n",
    "data[numerical_cols].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Data Processing\n",
    "\n",
    "We will look at the following functions:\n",
    "* .groupby()\n",
    "* .merge()\n",
    "* pd.concat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `.groupby()`\n",
    "\n",
    "Let's look again at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we're interested in getting the average of sales per month. We can use the function `.groupby()`, which returns a groupby object. On the groupby object, we can perform some sort of aggregation to get an interpretable result. Example of aggregations: `.mean(), .max(), .min(), .sum(), .count(), .nunique()`\n",
    "We will use here the `.mean()` for aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.groupby('MONTH_ID').mean() # return the grouped mean for each column \n",
    "data.groupby('MONTH_ID')['SALES'].mean() # returns the grouped mean for one column\n",
    "#data.groupby('MONTH_ID')['SALES'].aggregate([np.mean, np.sum]) # or you can define your own function and pass it in here\n",
    "\n",
    "# number of orders for each month\n",
    "# each row represents an item of an order, \n",
    "# this is why we might have multiple rows with the same order number\n",
    "# data.groupby('MONTH_ID')['ORDERNUMBER'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that by default NA values are excluded from group keys during the groupby operation. \n",
    "\n",
    "We can also group by multiple columns by passing them as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby(['YEAR_ID', 'MONTH_ID'])['SALES'].mean() # note the hierarchical indexing\n",
    "\n",
    "#data.groupby(['YEAR_ID', 'MONTH_ID'])['SALES'].mean().reset_index() \n",
    "   #transforms what you have as index to columns in your dataframe "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging DataFrames `.merge()`\n",
    "\n",
    "The function `.merge()`combines dataframes based on common columns. It performs in a similar way of join tables in SQL. In fact, left, right, outer, and inner joins work the same way here. [Link to Pandas user guide on merging](https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = pd.DataFrame([[11, 2004, 100], \n",
    "                       [14, 2005, 200], \n",
    "                       [13, 2006, 75], \n",
    "                       [11, 2007, 90]], \n",
    "                       columns=['user_id', 'order_id', 'cost'])\n",
    "\n",
    "clients = pd.DataFrame([[11, \"Jane\", \"Milwaukee\"], \n",
    "                        [12, \"John\", \"Chicago\"], \n",
    "                        [13,\"Sara\", \"Los Angeles\"],\n",
    "                        [14, \"Mike\", \"Austin\"]], \n",
    "                        columns=['user_id', 'name', 'city'])\n",
    "\n",
    "print(\"orders = \\n{}\\n\\nclients =\\n{}\\n\".format(orders, clients))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.merge(clients, left_on=\"user_id\",\n",
    "             right_on=\"user_id\", how=\"left\") # use keys from left frame only\n",
    "\n",
    "#orders.merge(clients, left_on=\"user_id\",\n",
    "#             right_on=\"user_id\", how=\"right\") # use keys from right frame only\n",
    "\n",
    "#orders.merge(clients, left_on=\"user_id\",\n",
    "#             right_on=\"user_id\", how=\"inner\") # use intersection of keys \n",
    "\n",
    "#orders.merge(clients, left_on=\"user_id\",\n",
    "#             right_on=\"user_id\", how=\"outer\") # use union of keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenating DataFrames `.concat()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vertical concatenation (useful when we want to concatenate data frames with common columns)\n",
    "df1 = pd.DataFrame({'Col1': range(5), 'Col2': range(-5,0,1), 'Col3': range(14,19)}, index=range(5))\n",
    "df2 = pd.DataFrame({'Col1': range(5), 'Col2': range(1,6), 'Col4': range(7,12)}, index=range(5,10))\n",
    "print(df1,\"\\n\\n\", df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df1, df2]) #common columns will be aligned, rows are appended\n",
    "#pd.concat([df1, df2], join=\"inner\") #keep common columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#horizontal concatenation (useful when we want to concatenate data frames with common index)\n",
    "df1 = pd.DataFrame({'Col1': range(5), 'Col2': range(-5,0,1), 'Col3': range(14,19)})\n",
    "df2 = pd.DataFrame({'Col4': range(5), 'Col5': range(1,6), 'Col6': range(7,12)})\n",
    "print(df1,\"\\n\\n\", df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df1, df2], axis=1) # rows with common indices are aligned, columns are appended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pivottable\n",
    "#crosstab\n",
    "#unstack, stack\n",
    "\n",
    "#sortvalues\n",
    "#rename columns\n",
    "\n",
    "#isnull()\n",
    "#dropna()\n",
    "#fillna()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataScienceEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
